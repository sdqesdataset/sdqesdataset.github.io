---
layout: default
---

<!-- <div class="posts">
  {% for post in site.posts %}
    <article class="post">

      <h1><a href="{{ site.baseurl }}{{ post.url }}">{{ post.title }}</a></h1>

      <div class="entry">
        {{ post.excerpt }}
      </div>

      <a href="{{ site.baseurl }}{{ post.url }}" class="read-more">Read More</a>
    </article>
  {% endfor %}
</div> -->

<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Streaming Detection of Query Event Start</title>
  
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" type="image/png" href="./assets/favicon.png">
  
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>
<body>
  
<section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Streaming Detection of Query Event Start</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://ceyzaguirre4.github.io">Cristobal Eyzaguirre</a><sup>1</sup></span>,
              <span class="author-block">
                <a href="https://erictang000.github.io">Eric Tang</a><sup>1</sup></span>,
              <span class="author-block">
                <a href="https://cs.stanford.edu/~shyamal/">Shyamal Buch</a><sup>1</sup></span>,
              <span class="author-block">
                <a href="https://adriengaidon.com">Adrien Gaidon</a><sup>1</sup></span>,
              <span class="author-block">
                  <a href="https://jiajunwu.com">Jiajun Wu</a><sup>1</sup></span>,
              <span class="author-block">
                <a href="https://www.niebles.net">Juan Carlos Niebles</a><sup>1</sup></span>
            </div>
  
            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Stanford University</span>
            </div>
          </div>
        </div>
      </div>
    </div>
</section>

<!-- Download Dataset button (centered)-->
<section class="section">
  <div class="container has-text-centered">
    <a href="https://sdqesdataset.github.io/dataset/dataset.zip" class="button is-primary is-large download-button">
      <i class="fas fa-download"></i> Download Dataset
    </a>
  </div>
</section>

<!-- Abstract -->
<div class="container is-max-desktop">
  <section class="section">
    <h2 class="title is-3">Abstract</h2>
    <p>
      Robotics, autonomous driving, augmented reality, and many embodied computer vision applications must quickly react to user-defined events unfolding in real time. We address this setting by proposing a novel task for multimodal video understanding---Streaming Detection of Quert Event Start (SDQES). The goal of SDQES is to identify the beginning of a complex event as described by a natural language query, with high accuracy and low latency.  We introduce a new benchmark based on the Ego4D dataset, as well as new task-specific metrics to study streaming multimodal detection of diverse events in an egocentric video setting. Inspired by parameter-efficient fine-tuning methods in NLP and for video tasks, we propose adapter-based baselines that enable image-to-video transfer learning, allowing for efficient online video modeling. We evaluate three vision-language backbones and three adapter architectures in both short-clip and untrimmed video settings.
  </p>
  </section>
  
<section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <!-- Add a GIF -->
        <figure>
          <!-- <img src="./assets/teaser.gif" alt="Teaser"> -->
          <img src="./static/Figure_1_sdqes.pdf" alt="Teaser">
      </div>
  
      <p class="caption">
        <b>Overview of our proposed SDQES task.</b> The goal of streaming detection of queried event start (SDQES) is for a system to detect the start of a complex event, described by natural language, with low latency from a streaming video input. This task is a novel intersection of multimodal event and online/streaming video understanding benchmarks. It is intended to encourage the design of new streaming multimodal models for challenging egocentric or embodied settings (e.g., assistive robotics, augmented reality) where time-sensitivity is a key concern for safety, accessibility, or convenience.
      </p>
    </div>
</section>


<div class="container is-max-desktop">
  <section class="section">
    <h2 class="title is-3">Contents</h2>
    <p>
      Each instance consists of raw video in .mp4 format coupled with temporally and contextually relevant natural language queries. Each query is accompanied by start and end timestamps in seconds encoded as floating point numbers and a response string.
      The code for generating the instances is available <a href="https://github.com/sdqesdataset/sdqes_generation">here</a>.
  </p>
  <p>
    EgoSDQES has a total of 12767 query annotations spanning 1773 distinct videos divided into training and validation splits such that there is no overlap between training and validation instances. This split is done according to the official Ego4D splits to avoid data contamination when evaluating models trained on Ego4D.
    The query and response annotations (along with the respective timestamps) is available for inspection and download <a href="https://sdqesdataset.github.io/dataset/all.csv">here</a>.
  </p>

  <p>
    We take special care to disambiguate repeat instances of events in the queries. Ie. if the event in the query has occurred before we make sure to add additional context (for example, prior context) to the query to make it unique. See the second row in the figure below for an example.
  </p>
  <figure>
  <img src="./static/dataset_examples.pdf" alt="Dataset Examples">
  <p class="caption">
    <b>Example videos and queries</b> from our dataset EgoSDQES. Each row shows a video frame with 3 queries. The start and end timestamps (mm:ss) of the query are shown in the top right corner of the frame.
  </p>
  </figure>
  </section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{eyzaguirre2024sdqes,
  title={Streaming Detection of Query Event Start},
  author={Eyzaguirre, Cristobal and Tang, Eric and Buch, Shyamal and Gaidon, Adrien and Wu, Jiajun and Niebles, Juan Carlos},
  booktitle={arxiv},
  year={2024}
}</code></pre>
  </div>
</section>


</body>
</html>